{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12980c8-ea85-409f-8c61-7560bba5ecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################Load External Libraries  ####################################\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import preprocessing, metrics\n",
    "from ipywidgets import widgets, interactive\n",
    "import gc\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime, timedelta \n",
    "from typing import Union\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "from itertools import cycle\n",
    "import datetime as dt\n",
    "from torch.autograd import Variable\n",
    "import random \n",
    "from matplotlib.pyplot import figure\n",
    "from fastprogress import master_bar, progress_bar\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import time \n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import os\n",
    "from datetime import datetime, time\n",
    "import torch.nn.functional as F\n",
    "from sklearn.svm import SVR\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import math\n",
    "import onnx\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17da9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Module.feat_engineer import time_ratio, sliding_windows\n",
    "from Module.data_clean import data_filter_svr, plot_data_before_after\n",
    "from Module.Time_alignment import shift_and_remove_bottom_rows, slide_y, slide_y_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df569b-06fa-4d32-a163-382fc557b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Model.CNN_BiLSTM import CNNLSTMModel, init_weights\n",
    "from Model.CNN_Transformer import CustomTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101daf35-8a76-4a8b-8bf8-3803ecfc5beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "path = r\"data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8d5c-82fe-4a38-ad4a-0159457a86b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1day\n",
    "window_size = 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c0de9-ed2d-4de2-8562-031739e7bd24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "input, output = [], []\n",
    "station = []\n",
    "xlist, ylist = [], []\n",
    "input.append( path + 'A.csv')\n",
    "output.append( path + '2023-measured-A.csv')\n",
    "\n",
    "# Data regularization\n",
    "for i in range(1):\n",
    "    I = pd.read_csv(input[0]) # changeable\n",
    "    # I = I.to_frame()\n",
    "    I.set_index(I.columns[0], inplace=True)\n",
    "    I.interpolate(method='linear', axis=0, inplace=True)\n",
    "    \n",
    "    O = pd.read_csv(output[0]).iloc[:, [1, 2]]\n",
    "    O.set_index(O.columns[0], inplace=True)\n",
    "    O.interpolate(method='linear', axis=0, inplace=True)\n",
    "    p = I.join(O, how='outer') # Merge by date\n",
    "    print(I.shape,O.shape)\n",
    "    \n",
    "    # Move the data in the last column down the window_size row\n",
    "    p.iloc[:, -1] = p.iloc[:, -1].shift(-window_size)\n",
    "    p.dropna(axis=0, inplace = True)\n",
    "\n",
    "    # data clean\n",
    "    p_before = p\n",
    "    p = data_filter_svr(p)\n",
    "    p_after = p\n",
    "\n",
    "    # Min-Max processing\n",
    "    original_max, original_min = p.iloc[:, -1].max(), p.iloc[:, -1].min() # original range recorded\n",
    "    scaler = MinMaxScaler()\n",
    "    p_ = pd.DataFrame(scaler.fit_transform(p), columns=p.columns)\n",
    "\n",
    "    # time ratio column constructed\n",
    "    time_index = p.index.to_numpy()\n",
    "    p_['time'] = time_ratio(time_index)\n",
    "    station.append((p_, time_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782900c4-0476-47ec-99cb-0d24800c220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1):\n",
    "    x, y = sliding_windows(station[i][0], station[i][1], window_size)\n",
    "    print(i,':','x---',x.shape,'---','y---',y.shape)\n",
    "    xlist.append(x)\n",
    "    ylist.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51f64ae-e30f-4d0e-b588-0c7856e63673",
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = x[:, :, [0, 1, 2, 6]] # model-prediction\n",
    "x2 = x[:, :, [3, 4, 5, 6]] # station-measured (without time_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70302fa8-5dc9-434f-9367-2eaa81452bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(y) * 0.7)\n",
    "test_size = len(y) - train_size\n",
    "\n",
    "dataX = Variable(torch.Tensor(np.array(x1)))\n",
    "dataY = Variable(torch.Tensor(np.array(y)))\n",
    "\n",
    "trainX = Variable(torch.Tensor(np.array(x1[0:train_size])))\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size])))\n",
    "\n",
    "testX = Variable(torch.Tensor(np.array(x1[train_size:len(x1)])))\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb41584-0237-40ee-9c0f-d4fcbc66e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"train shape is:\",trainX.size())\n",
    "print(\"train label shape is:\",trainY.size())\n",
    "print(\"test shape is:\",testX.size())\n",
    "print(\"test label shape is:\",testY.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece4d545",
   "metadata": {},
   "source": [
    "## Short-term Prediction (CNN-LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fedba2e2-b261-41ce-8268-99712807bc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNNLSTM\n",
    "num_epochs = 2000 \n",
    "input_dims = 4\n",
    "lstm_units = 38\n",
    "learning_rate = 0.04 \n",
    "momentum=0.88# 0.9 # SGDM, empirical \n",
    "weight_decay=1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e948b3d-1798-4d55-ac64-432d2f85a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = CNNLSTMModel(input_dims, lstm_units)\n",
    "lstm.to(device)\n",
    "lstm.apply(init_weights)\n",
    "\n",
    "criterion = torch.nn.L1Loss().to(device) # MAE\n",
    "optimizer = torch.optim.SGD(lstm.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n",
    "# When there is no improvement within a certain number of epochs, the learning rate will be automatically reduced.\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer,  patience=100, factor =0.1 ,min_lr=1e-7, eps=1e-08) \n",
    "\n",
    "\n",
    "# Train CNN-LSTM MODEL\n",
    "epoch_list = []\n",
    "train_loss_list = []\n",
    "valid_loss_list = []\n",
    "\n",
    "for epoch in progress_bar(range(num_epochs)):\n",
    "    lstm.train()\n",
    "    outputs = lstm(trainX.to(device))\n",
    "    optimizer.zero_grad()\n",
    "    torch.nn.utils.clip_grad_norm_(lstm.parameters(), 1)\n",
    "    # loss function\n",
    "    loss = criterion(outputs, trainY.to(device))\n",
    "    loss.backward()\n",
    "    scheduler.step(loss)\n",
    "    optimizer.step()\n",
    "    lstm.eval()\n",
    "    valid = lstm(testX.to(device))\n",
    "    vall_loss = criterion(valid, testY.to(device))\n",
    "    scheduler.step(vall_loss)\n",
    "    \n",
    "    # loss value\n",
    "    epoch_list.append(epoch + 1)\n",
    "    train_loss_list.append(loss.cpu().item())\n",
    "    valid_loss_list.append(vall_loss.cpu().item())\n",
    "    \n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        print(\"Epoch: %d, loss: %1.5f valid loss:  %1.5f \" %(epoch + 1, loss.cpu().item(), vall_loss.cpu().item()))\n",
    "\n",
    "# plot\n",
    "plt.figure()\n",
    "plt.plot(epoch_list, train_loss_list, label='Train Loss')\n",
    "plt.plot(epoch_list, valid_loss_list, label='Valid Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "torch.save(lstm, 'y_for_AT.pth') # Intermediate power prediction (Mode training result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfbecdd-a245-45f6-ba6f-48abf8861bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_module(lstm, dataX =  dataX, testX = testX):\n",
    "    \n",
    "    # employing the trained model to predict\n",
    "    lstm.eval()\n",
    "    train_predict = lstm(dataX.to(device))\n",
    "    data_pre = train_predict.cpu().data.numpy()\n",
    "    dataY_pl = dataY.data.numpy()\n",
    "\n",
    "    # denormalization\n",
    "    data_predict  = data_pre * (original_max - original_min) + original_min\n",
    "    dataY_plot = dataY_pl * (original_max - original_min) + original_min\n",
    "\n",
    "    # predicted output\n",
    "    print(data_predict.shape)\n",
    "    df_predict = pd.DataFrame(data_predict)\n",
    "\n",
    "    # actual output\n",
    "    df_labels = pd.DataFrame(dataY_plot)\n",
    "\n",
    "    # plot（Entire）\n",
    "    figure(num=None, figsize=(19, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(df_predict[0])\n",
    "    plt.legend(['Prediction','Time Series'],fontsize = 21)\n",
    "    plt.suptitle('Time-Series Prediction Entire Set',fontsize = 23)\n",
    "    plt.xticks(fontsize=21 )\n",
    "    plt.yticks(fontsize=21 )\n",
    "    plt.ylabel(ylabel = 'Photovoltaic Power Generation',fontsize = 21)\n",
    "    plt.show()\n",
    "\n",
    "    # plot（Test）\n",
    "    figure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "    plt.plot(df_labels.iloc[-testX.size()[0]:][0].to_numpy())\n",
    "    plt.plot(df_predict.iloc[-testX.size()[0]:][0].to_numpy())\n",
    "    plt.legend(['Actual Output','Predicted Output'],fontsize = 21)\n",
    "    plt.suptitle('CNN+BiLSTM(model only)',fontsize = 23)\n",
    "    plt.xticks(fontsize=21 )\n",
    "    plt.yticks(fontsize=21 )\n",
    "    plt.xlabel(xlabel = 'Test Set Data Points',fontsize = 21)\n",
    "    plt.ylabel(ylabel = 'Ouput',fontsize = 21)\n",
    "    plt.savefig('short_north_fzbl.png',dpi=1200)\n",
    "    plt.show()\n",
    "\n",
    "    # test shape\n",
    "    print('testX:', testX.size())\n",
    "    # RMSE\n",
    "    print('RMSE:', np.sqrt(mean_squared_error(dataY_plot[-testX.size()[0]:],data_predict[-testX.size()[0]:])))\n",
    "    # MAE\n",
    "    print('MAE:', mean_absolute_error(dataY_plot[-testX.size()[0]:],data_predict[-testX.size()[0]:]))\n",
    "    return dataY_plot, data_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ebb91-5e6a-4d78-a1de-afba3d3e1a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out 'y_for_AT'\n",
    "fusion = torch.load('y_for_AT.pth')\n",
    "print('\\n'+'-'*10+'Intermediate power prediction (Model prediction)'+'-'*10+'\\n')\n",
    "actual, y_for_AT = predict_module(fusion)\n",
    "y_for_AT = list((y_for_AT - original_min)/(original_max - original_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed30cd0d-6fb8-44cc-8ee7-f26c9baa09a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x2[window_size:-window_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6de561-b979-4262-99a4-af0f4c2be3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = slide_y(y_for_AT, window_size)\n",
    "y_pre = slide_y_pre(y, window_size)\n",
    "print(Y.shape,y_pre.shape,x2.shape)\n",
    "x2= np.concatenate((x2, y_pre, Y), axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a5d99-2645-4534-9e23-75343c5242bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y[window_size:-window_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4ddafc-f9de-4bb7-8dfe-72e00477b0ba",
   "metadata": {},
   "source": [
    "## Multi-head attention cross-attention\n",
    "[allowing historical data (q, k) to play a correction role]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d611466f-954e-475f-85a5-1a3cbd79be27",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2 \n",
    "d_model = 64   \n",
    "num_heads = 32   \n",
    "d_ff = 256    \n",
    "max_len = 50000  \n",
    "num_layers = 2  \n",
    "output_dim = 1  \n",
    "seq_len = 16 # 24【6h】\n",
    "num_epochs = 500\n",
    "learning_rate = 0.001 # 1e-3\n",
    "batch_size = 64\n",
    "weight_decay = 0.0001 #1e-4\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "test_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025cc25d-86d8-4c63-9fa3-9c595abb25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split for station data\n",
    "train_size -= window_size\n",
    "x2 = shift_and_remove_bottom_rows(x2, seq_len) # adjust to output in the past\n",
    "staX = Variable(torch.Tensor(np.array(x2))).to(device)\n",
    "sta_trainX = Variable(torch.Tensor(np.array(x2[0:train_size]))).to(device)\n",
    "sta_testX = Variable(torch.Tensor(np.array(x2[train_size:len(x2)]))).to(device)\n",
    "print('staX',staX.size())\n",
    "print('sta_trainX',sta_trainX.size())\n",
    "print('sta_testX',sta_testX.size())\n",
    "\n",
    "y = y[:-seq_len]\n",
    "trainY = Variable(torch.Tensor(np.array(y[0:train_size]))).to(device)\n",
    "testY = Variable(torch.Tensor(np.array(y[train_size:len(y)]))).to(device)\n",
    "print(testY.shape,trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a47e22-5941-4ef5-a85d-b32c0f0e7afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train set\n",
    "x = sta_trainX[seq_len:-seq_len,-seq_len:,[0,1,2,3,4,-1]].to(device)  # shape (batch_size, seq_len, input_dim) \n",
    "y = slide_y_pre(trainY.to('cpu'),seq_len)      # shape (batch_size,seq_len)\n",
    "y = Variable(torch.Tensor(y.squeeze(-1))).to(device)\n",
    "print(x.shape,y.shape)\n",
    "\n",
    "# test set\n",
    "test_x = sta_testX[seq_len:-seq_len,-seq_len:,[0,1,2,3,4,-1]].to(device)  # 形状为 (batch_size, seq_len, input_dim)\n",
    "test_y1 =slide_y_pre(testY.to('cpu'),seq_len)      # shape: (batch_size,seq_len)\n",
    "test_y1 = Variable(torch.Tensor(test_y1.squeeze(-1))).to(device)\n",
    "print(test_x.shape,test_y1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bf11d7-1c11-4fd9-917e-0dd24032190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTransformer(d_model, num_heads, d_ff, seq_len)\n",
    "model.to(device)\n",
    "criterion = nn.L1Loss().to(device)\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.88, weight_decay=1e-3)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=100, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7300823-30a4-4f73-a21e-5a73793bf211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloaders\n",
    "dataset = TensorDataset(x, y)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# early stopping params\n",
    "patience = 10  # early stopping patience\n",
    "min_epochs_to_trigger = 200  # the minimum number of epochs before early stopping can be triggered\n",
    "best_loss = np.inf\n",
    "stable_epochs = 0\n",
    "threshold = 0.005 # loss threshold for early stopping\n",
    "\n",
    "# Train\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch_x, batch_y in dataloader:\n",
    "        batch_x = batch_x.to(device)\n",
    "        batch_y = batch_y.to(device)\n",
    "        # forward\n",
    "        output = model(batch_x)\n",
    "        # loss calculation\n",
    "        loss = criterion(output, batch_y)\n",
    "        epoch_loss += loss.item()\n",
    "        # backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # learning rate \n",
    "    scheduler.step(epoch_loss)\n",
    "    train_losses.append(epoch_loss / len(dataloader))\n",
    "    \n",
    "    # test loss calculated\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_x = test_x.to(device)\n",
    "        test_y1 = test_y1.to(device)\n",
    "        test_output = model(test_x)\n",
    "        test_loss = criterion(test_output, test_y1)\n",
    "        test_losses.append(test_loss.item())\n",
    "    \n",
    "    # check early-stopping condition\n",
    "    if epoch >= min_epochs_to_trigger:\n",
    "        if abs(test_loss - best_loss) < threshold:\n",
    "            stable_epochs += 1\n",
    "        else:\n",
    "            stable_epochs = 0\n",
    "\n",
    "        best_loss = min(best_loss, test_loss)\n",
    "\n",
    "        if stable_epochs >= patience:\n",
    "            print(f\"Early stopping triggered at epoch {epoch + 1} due to stable loss\")\n",
    "            break\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_losses[-1]:.4f}, Test Loss: {test_losses[-1]:.4f}')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), 'CA.pth')\n",
    "\n",
    "# plot loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "if valid_losses:\n",
    "    plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.plot(test_losses, label='Test Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training, Validation, and Test Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47dc409-50e3-4ec4-a686-195edba58028",
   "metadata": {},
   "outputs": [],
   "source": [
    "KEY = -1 # 8\n",
    "# load the model\n",
    "CustomTransformer(d_model, num_heads, d_ff, seq_len)\n",
    "model.load_state_dict(torch.load('CA.pth'))\n",
    "model.eval()\n",
    "\n",
    "# predict\n",
    "with torch.no_grad():\n",
    "    test_output1 = model(test_x)\n",
    "    test_loss = criterion(test_output1, test_y1)\n",
    "    valid_losses.append(test_loss.item())\n",
    "    print(f'Test Loss: {test_loss.item():.4f}')\n",
    "\n",
    "# denormalization\n",
    "test_output = test_output1 * (original_max - original_min) + original_min\n",
    "test_y = test_y1 * (original_max - original_min) + original_min\n",
    "\n",
    "test_output = test_output.to('cpu')[:,KEY]\n",
    "test_y = test_y.to('cpu')[:,KEY]\n",
    "print(test_output.shape,test_y.shape)\n",
    "\n",
    "# calculate RMSE and MAE\n",
    "rmse = np.sqrt(mean_squared_error(test_y, test_output))\n",
    "mae = mean_absolute_error(test_y, test_output)\n",
    "print(f'Test RMSE: {rmse:.4f}')\n",
    "print(f'Test MAE: {mae:.4f}')\n",
    "\n",
    "# compare predicted and actual values\n",
    "figure(num=None, figsize=(23, 6), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.plot(test_y.numpy(), label='True Values')\n",
    "plt.plot(test_output.numpy(), label='Predicted Values')\n",
    "plt.xticks(fontsize=21 )\n",
    "plt.yticks(fontsize=21 )\n",
    "plt.xlabel('Test Set Data Points',fontsize = 21)\n",
    "plt.ylabel('Output',fontsize=21)\n",
    "plt.title('CNN+Transformer(4h)',fontsize=21)\n",
    "plt.legend(['Actual Output','Predicted Output'],fontsize = 21)\n",
    "plt.savefig('CNN+Transformer(4h).png',dpi=1200)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
